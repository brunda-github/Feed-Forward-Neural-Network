# -*- coding: utf-8 -*-
"""FFN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hatHE-oqAWPZjaAWPexKdyzjkG2sh6oQ
"""

import numpy as np
import matplotlib.pyplot as plt
from keras.datasets import fashion_mnist

# Load Fashion-MNIST dataset
(trainDataInput, trainDataOutput), (testDataInput, testDataOutput) = fashion_mnist.load_data()

class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',
               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']

# Function to plot one sample image for each class
def plot_samples(images, labels, class_names):
    num_classes = len(class_names)
    fig, axes = plt.subplots(1, num_classes, figsize=(20, 20))
    for i in range(num_classes):
        #Get the list of indices where label i is present
        class_indices = np.where(labels == i)[0]
        #Use the first index and plot the respective image
        index = class_indices[0]
        axes[i].imshow(images[index], cmap='gray')
        axes[i].set_title(class_names[i])
        axes[i].axis('off')
    plt.show()

# Plot one sample image for each class
plot_samples(trainDataInput, trainDataOutput, class_names)

class FFN:
  def __init__(self, input, output, nHiddenLayers, nNeurons, activationfunc):
    self.x = input
    self.y = output
    self.nHiddenlayers = nHiddenLayers
    self.nNeurons = nNeurons
    self.activationfunc = activationfunc
    self.a = np.array(nHiddenLayers+1, dtype = object)
    self.h = np.array(nHiddenLayers+1, dtype = object)
    self.b = np.array(nHiddenLayers+1, dtype = object)
    self.w = np.array(nHiddenLayers+1, dtype = object)
    self.h[0] = input.flatten()
    self.ypred = np.array(self.y.shape)
    return

  def initWeights(self, type):
    if type == "random":
      for k in range(1, self.nHiddenLayers + 1):
        self.w[k] = np.random.randn(self.nNeurons[k], self.nNeurons[k-1])
    return

  def sigmoid(x):
    return 1/(1 + np.exp(x));

  def activationfunc_g(self,x):
    if self.activationfunc == "sigmoid":
      return 1 / (1 + np.exp(-x))
    elif self.activationfunc == "tanh":
      return (np.exp(x) - np.exp(-x)) / (np.exp(x) + np.exp(-x))
    elif self.activationfunc == "ReLU":
      return np.maximum(0,x)
    return

  def Outputfunc(x):
    #Classification problem so use softmax
    x = np.exp(x)
    x = x / np.sum(x)
    return x

  def forward_propogation(self):
    l = self.nHiddenLayers
    for k in range(1, l):
      self.a[k] = np.dot(self.w[k], self.h[k-1]) + self.b[k]
      self.h[k] = self.activationfunc_g(self.a[k])
    self.a[l] = np.dot(self.w[l], self.h[l-1]) + self.b[l]
    self.ypred = self.Outputfunc(self.a[l])